{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7497725,"sourceType":"datasetVersion","datasetId":4365847}],"dockerImageVersionId":30635,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-28T11:01:43.732680Z","iopub.execute_input":"2024-01-28T11:01:43.733401Z","iopub.status.idle":"2024-01-28T11:01:43.743939Z","shell.execute_reply.started":"2024-01-28T11:01:43.733364Z","shell.execute_reply":"2024-01-28T11:01:43.742719Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/input/creditcardfraud/Creditcard_data.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"cc=pd.read_csv(\"/kaggle/input/creditcardfraud/Creditcard_data.csv\")\nfrom imblearn.over_sampling import RandomOverSampler\nfrom imblearn.under_sampling import RandomUnderSampler\nx=cc.drop('Class',axis=1)\ny=cc['Class']\n\nros = RandomOverSampler(random_state=42)\nx_ros, y_ros = ros.fit_resample(x, y)\n\n\n\n# Create a new balanced dataframe\ncc_balanced= pd.concat([x_ros, y_ros], axis=1)\n\n# Save the balanced dataframe to a new CSV file\ncc_balanced.to_csv('Balanced_data.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-01-28T11:01:48.704311Z","iopub.execute_input":"2024-01-28T11:01:48.704836Z","iopub.status.idle":"2024-01-28T11:01:50.111988Z","shell.execute_reply.started":"2024-01-28T11:01:48.704794Z","shell.execute_reply":"2024-01-28T11:01:50.110259Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"cc_balanced","metadata":{"execution":{"iopub.status.busy":"2024-01-28T11:03:06.726790Z","iopub.execute_input":"2024-01-28T11:03:06.727275Z","iopub.status.idle":"2024-01-28T11:03:06.777271Z","shell.execute_reply.started":"2024-01-28T11:03:06.727218Z","shell.execute_reply":"2024-01-28T11:03:06.775741Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"      Time        V1        V2        V3        V4        V5        V6  \\\n0        0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388   \n1        0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361   \n2        1 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499   \n3        1 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203   \n4        2 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921   \n...    ...       ...       ...       ...       ...       ...       ...   \n1521   529 -2.000567 -2.495484  2.467149  1.140053  2.462010  0.594262   \n1522   164  0.073497  0.551033  0.451890  0.114964  0.822947  0.251480   \n1523   164  0.073497  0.551033  0.451890  0.114964  0.822947  0.251480   \n1524   539 -1.738582  0.052740  1.187057 -0.656652  0.920623 -0.291788   \n1525   472 -3.043541 -3.157307  1.088463  2.288644  1.359805 -1.064823   \n\n            V7        V8        V9  ...       V21       V22       V23  \\\n0     0.239599  0.098698  0.363787  ... -0.018307  0.277838 -0.110474   \n1    -0.078803  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288   \n2     0.791461  0.247676 -1.514654  ...  0.247998  0.771679  0.909412   \n3     0.237609  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321   \n4     0.592941 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458   \n...        ...       ...       ...  ...       ...       ...       ...   \n1521 -2.110183  0.788347  0.958809  ...  0.422452  1.195394  0.297836   \n1522  0.296319  0.139497 -0.123050  ... -0.128758 -0.381932  0.151012   \n1523  0.296319  0.139497 -0.123050  ... -0.128758 -0.381932  0.151012   \n1524  0.269083  0.140631  0.023464  ... -0.179545 -0.192036 -0.261879   \n1525  0.325574 -0.067794 -0.270953  ...  0.661696  0.435477  1.375966   \n\n           V24       V25       V26       V27       V28  Amount  Class  \n0     0.066928  0.128539 -0.189115  0.133558 -0.021053  149.62      0  \n1    -0.339846  0.167170  0.125895 -0.008983  0.014724    2.69      1  \n2    -0.689281 -0.327642 -0.139097 -0.055353 -0.059752  378.66      0  \n3    -1.175575  0.647376 -0.221929  0.062723  0.061458  123.50      0  \n4     0.141267 -0.206010  0.502292  0.219422  0.215153   69.99      0  \n...        ...       ...       ...       ...       ...     ...    ...  \n1521 -0.857105 -0.219322  0.861019 -0.124622 -0.171060    1.50      1  \n1522 -1.363967 -1.389079  0.075412  0.231750  0.230171    0.99      1  \n1523 -1.363967 -1.389079  0.075412  0.231750  0.230171    0.99      1  \n1524 -0.237477 -0.335040  0.240323 -0.345129 -0.383563    1.00      1  \n1525 -0.293803  0.279798 -0.145362 -0.252773  0.035764  529.00      1  \n\n[1526 rows x 31 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Time</th>\n      <th>V1</th>\n      <th>V2</th>\n      <th>V3</th>\n      <th>V4</th>\n      <th>V5</th>\n      <th>V6</th>\n      <th>V7</th>\n      <th>V8</th>\n      <th>V9</th>\n      <th>...</th>\n      <th>V21</th>\n      <th>V22</th>\n      <th>V23</th>\n      <th>V24</th>\n      <th>V25</th>\n      <th>V26</th>\n      <th>V27</th>\n      <th>V28</th>\n      <th>Amount</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>-1.359807</td>\n      <td>-0.072781</td>\n      <td>2.536347</td>\n      <td>1.378155</td>\n      <td>-0.338321</td>\n      <td>0.462388</td>\n      <td>0.239599</td>\n      <td>0.098698</td>\n      <td>0.363787</td>\n      <td>...</td>\n      <td>-0.018307</td>\n      <td>0.277838</td>\n      <td>-0.110474</td>\n      <td>0.066928</td>\n      <td>0.128539</td>\n      <td>-0.189115</td>\n      <td>0.133558</td>\n      <td>-0.021053</td>\n      <td>149.62</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>1.191857</td>\n      <td>0.266151</td>\n      <td>0.166480</td>\n      <td>0.448154</td>\n      <td>0.060018</td>\n      <td>-0.082361</td>\n      <td>-0.078803</td>\n      <td>0.085102</td>\n      <td>-0.255425</td>\n      <td>...</td>\n      <td>-0.225775</td>\n      <td>-0.638672</td>\n      <td>0.101288</td>\n      <td>-0.339846</td>\n      <td>0.167170</td>\n      <td>0.125895</td>\n      <td>-0.008983</td>\n      <td>0.014724</td>\n      <td>2.69</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>-1.358354</td>\n      <td>-1.340163</td>\n      <td>1.773209</td>\n      <td>0.379780</td>\n      <td>-0.503198</td>\n      <td>1.800499</td>\n      <td>0.791461</td>\n      <td>0.247676</td>\n      <td>-1.514654</td>\n      <td>...</td>\n      <td>0.247998</td>\n      <td>0.771679</td>\n      <td>0.909412</td>\n      <td>-0.689281</td>\n      <td>-0.327642</td>\n      <td>-0.139097</td>\n      <td>-0.055353</td>\n      <td>-0.059752</td>\n      <td>378.66</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>-0.966272</td>\n      <td>-0.185226</td>\n      <td>1.792993</td>\n      <td>-0.863291</td>\n      <td>-0.010309</td>\n      <td>1.247203</td>\n      <td>0.237609</td>\n      <td>0.377436</td>\n      <td>-1.387024</td>\n      <td>...</td>\n      <td>-0.108300</td>\n      <td>0.005274</td>\n      <td>-0.190321</td>\n      <td>-1.175575</td>\n      <td>0.647376</td>\n      <td>-0.221929</td>\n      <td>0.062723</td>\n      <td>0.061458</td>\n      <td>123.50</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2</td>\n      <td>-1.158233</td>\n      <td>0.877737</td>\n      <td>1.548718</td>\n      <td>0.403034</td>\n      <td>-0.407193</td>\n      <td>0.095921</td>\n      <td>0.592941</td>\n      <td>-0.270533</td>\n      <td>0.817739</td>\n      <td>...</td>\n      <td>-0.009431</td>\n      <td>0.798278</td>\n      <td>-0.137458</td>\n      <td>0.141267</td>\n      <td>-0.206010</td>\n      <td>0.502292</td>\n      <td>0.219422</td>\n      <td>0.215153</td>\n      <td>69.99</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1521</th>\n      <td>529</td>\n      <td>-2.000567</td>\n      <td>-2.495484</td>\n      <td>2.467149</td>\n      <td>1.140053</td>\n      <td>2.462010</td>\n      <td>0.594262</td>\n      <td>-2.110183</td>\n      <td>0.788347</td>\n      <td>0.958809</td>\n      <td>...</td>\n      <td>0.422452</td>\n      <td>1.195394</td>\n      <td>0.297836</td>\n      <td>-0.857105</td>\n      <td>-0.219322</td>\n      <td>0.861019</td>\n      <td>-0.124622</td>\n      <td>-0.171060</td>\n      <td>1.50</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1522</th>\n      <td>164</td>\n      <td>0.073497</td>\n      <td>0.551033</td>\n      <td>0.451890</td>\n      <td>0.114964</td>\n      <td>0.822947</td>\n      <td>0.251480</td>\n      <td>0.296319</td>\n      <td>0.139497</td>\n      <td>-0.123050</td>\n      <td>...</td>\n      <td>-0.128758</td>\n      <td>-0.381932</td>\n      <td>0.151012</td>\n      <td>-1.363967</td>\n      <td>-1.389079</td>\n      <td>0.075412</td>\n      <td>0.231750</td>\n      <td>0.230171</td>\n      <td>0.99</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1523</th>\n      <td>164</td>\n      <td>0.073497</td>\n      <td>0.551033</td>\n      <td>0.451890</td>\n      <td>0.114964</td>\n      <td>0.822947</td>\n      <td>0.251480</td>\n      <td>0.296319</td>\n      <td>0.139497</td>\n      <td>-0.123050</td>\n      <td>...</td>\n      <td>-0.128758</td>\n      <td>-0.381932</td>\n      <td>0.151012</td>\n      <td>-1.363967</td>\n      <td>-1.389079</td>\n      <td>0.075412</td>\n      <td>0.231750</td>\n      <td>0.230171</td>\n      <td>0.99</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1524</th>\n      <td>539</td>\n      <td>-1.738582</td>\n      <td>0.052740</td>\n      <td>1.187057</td>\n      <td>-0.656652</td>\n      <td>0.920623</td>\n      <td>-0.291788</td>\n      <td>0.269083</td>\n      <td>0.140631</td>\n      <td>0.023464</td>\n      <td>...</td>\n      <td>-0.179545</td>\n      <td>-0.192036</td>\n      <td>-0.261879</td>\n      <td>-0.237477</td>\n      <td>-0.335040</td>\n      <td>0.240323</td>\n      <td>-0.345129</td>\n      <td>-0.383563</td>\n      <td>1.00</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1525</th>\n      <td>472</td>\n      <td>-3.043541</td>\n      <td>-3.157307</td>\n      <td>1.088463</td>\n      <td>2.288644</td>\n      <td>1.359805</td>\n      <td>-1.064823</td>\n      <td>0.325574</td>\n      <td>-0.067794</td>\n      <td>-0.270953</td>\n      <td>...</td>\n      <td>0.661696</td>\n      <td>0.435477</td>\n      <td>1.375966</td>\n      <td>-0.293803</td>\n      <td>0.279798</td>\n      <td>-0.145362</td>\n      <td>-0.252773</td>\n      <td>0.035764</td>\n      <td>529.00</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>1526 rows × 31 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"#Creating a random sample using random sampling\nnp.random.seed(0)\n\nsample_size=int((pow(1.96,2)*0.5*0.5)/0.0025)\n\nrandom_sample = cc_balanced.sample(n=sample_size, random_state=0)","metadata":{"execution":{"iopub.status.busy":"2024-01-28T11:03:39.590437Z","iopub.execute_input":"2024-01-28T11:03:39.590862Z","iopub.status.idle":"2024-01-28T11:03:39.600152Z","shell.execute_reply.started":"2024-01-28T11:03:39.590830Z","shell.execute_reply":"2024-01-28T11:03:39.598505Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"random_sample.head()","metadata":{"execution":{"iopub.status.busy":"2024-01-28T11:03:51.661832Z","iopub.execute_input":"2024-01-28T11:03:51.662358Z","iopub.status.idle":"2024-01-28T11:03:51.695455Z","shell.execute_reply.started":"2024-01-28T11:03:51.662324Z","shell.execute_reply":"2024-01-28T11:03:51.694323Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"      Time        V1        V2        V3        V4        V5        V6  \\\n1361   484 -0.928088  0.398194  1.741131  0.182673  0.966387 -0.901004   \n511    377  1.166919  0.027049  0.513875  0.860965 -0.519452 -0.681147   \n9        9 -0.338262  1.119593  1.044367 -0.222187  0.499361 -0.246761   \n393    284 -0.810756  0.654499  2.217257  0.104341 -0.286801  0.117833   \n471    346  1.077079  0.284980  0.007731  1.657073  0.052020  0.446389   \n\n            V7        V8        V9  ...       V21       V22       V23  \\\n1361  0.879016 -0.156590 -0.142117  ...  0.066353  0.281378 -0.257966   \n511   0.074992 -0.187776  0.345399  ... -0.202750 -0.441391 -0.025782   \n9     0.651583  0.069539 -0.736727  ... -0.246914 -0.633753 -0.120794   \n393   0.287552 -0.736461  0.699092  ...  0.938194  0.571651 -0.101609   \n471  -0.407036  0.355704  0.626039  ... -0.174337 -0.174161 -0.153375   \n\n           V24       V25       V26       V27       V28  Amount  Class  \n1361  0.385384  0.391117 -0.453853 -0.104448 -0.125765    1.00      1  \n511   0.452607  0.467223  0.262577 -0.023834  0.020521   40.83      0  \n9    -0.385050 -0.069733  0.094199  0.246219  0.083076    3.68      0  \n393   0.363928 -0.170947 -0.471524  0.058958 -0.079157   30.30      0  \n471  -0.466331  0.611001 -0.252871  0.090375  0.054820   10.99      0  \n\n[5 rows x 31 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Time</th>\n      <th>V1</th>\n      <th>V2</th>\n      <th>V3</th>\n      <th>V4</th>\n      <th>V5</th>\n      <th>V6</th>\n      <th>V7</th>\n      <th>V8</th>\n      <th>V9</th>\n      <th>...</th>\n      <th>V21</th>\n      <th>V22</th>\n      <th>V23</th>\n      <th>V24</th>\n      <th>V25</th>\n      <th>V26</th>\n      <th>V27</th>\n      <th>V28</th>\n      <th>Amount</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1361</th>\n      <td>484</td>\n      <td>-0.928088</td>\n      <td>0.398194</td>\n      <td>1.741131</td>\n      <td>0.182673</td>\n      <td>0.966387</td>\n      <td>-0.901004</td>\n      <td>0.879016</td>\n      <td>-0.156590</td>\n      <td>-0.142117</td>\n      <td>...</td>\n      <td>0.066353</td>\n      <td>0.281378</td>\n      <td>-0.257966</td>\n      <td>0.385384</td>\n      <td>0.391117</td>\n      <td>-0.453853</td>\n      <td>-0.104448</td>\n      <td>-0.125765</td>\n      <td>1.00</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>511</th>\n      <td>377</td>\n      <td>1.166919</td>\n      <td>0.027049</td>\n      <td>0.513875</td>\n      <td>0.860965</td>\n      <td>-0.519452</td>\n      <td>-0.681147</td>\n      <td>0.074992</td>\n      <td>-0.187776</td>\n      <td>0.345399</td>\n      <td>...</td>\n      <td>-0.202750</td>\n      <td>-0.441391</td>\n      <td>-0.025782</td>\n      <td>0.452607</td>\n      <td>0.467223</td>\n      <td>0.262577</td>\n      <td>-0.023834</td>\n      <td>0.020521</td>\n      <td>40.83</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>9</td>\n      <td>-0.338262</td>\n      <td>1.119593</td>\n      <td>1.044367</td>\n      <td>-0.222187</td>\n      <td>0.499361</td>\n      <td>-0.246761</td>\n      <td>0.651583</td>\n      <td>0.069539</td>\n      <td>-0.736727</td>\n      <td>...</td>\n      <td>-0.246914</td>\n      <td>-0.633753</td>\n      <td>-0.120794</td>\n      <td>-0.385050</td>\n      <td>-0.069733</td>\n      <td>0.094199</td>\n      <td>0.246219</td>\n      <td>0.083076</td>\n      <td>3.68</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>393</th>\n      <td>284</td>\n      <td>-0.810756</td>\n      <td>0.654499</td>\n      <td>2.217257</td>\n      <td>0.104341</td>\n      <td>-0.286801</td>\n      <td>0.117833</td>\n      <td>0.287552</td>\n      <td>-0.736461</td>\n      <td>0.699092</td>\n      <td>...</td>\n      <td>0.938194</td>\n      <td>0.571651</td>\n      <td>-0.101609</td>\n      <td>0.363928</td>\n      <td>-0.170947</td>\n      <td>-0.471524</td>\n      <td>0.058958</td>\n      <td>-0.079157</td>\n      <td>30.30</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>471</th>\n      <td>346</td>\n      <td>1.077079</td>\n      <td>0.284980</td>\n      <td>0.007731</td>\n      <td>1.657073</td>\n      <td>0.052020</td>\n      <td>0.446389</td>\n      <td>-0.407036</td>\n      <td>0.355704</td>\n      <td>0.626039</td>\n      <td>...</td>\n      <td>-0.174337</td>\n      <td>-0.174161</td>\n      <td>-0.153375</td>\n      <td>-0.466331</td>\n      <td>0.611001</td>\n      <td>-0.252871</td>\n      <td>0.090375</td>\n      <td>0.054820</td>\n      <td>10.99</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 31 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"#Now we will aplly 5 different models to our random sample\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\n\n\n\nfrom sklearn.metrics import accuracy_score\nx_random= random_sample.drop(columns=['Class'])\ny_random = random_sample['Class']\nx_random_train, x_random_test,y_random_train, y_random_test = train_test_split(x_random,y_random, random_state=132, \n                                   test_size=0.25, \n                                   shuffle=True)\n\n\n\n# Train and evaluate the models\nmodels = {\n    \"Logistic Regression\": LogisticRegression(),\n    \"Gradient Boosting\": GradientBoostingClassifier(),\n    \"Naive bayes Classifier\":GaussianNB(),\n    \"Decision Tree\": DecisionTreeClassifier(),\n    \"Random Forest\": RandomForestClassifier()  \n}\nacc_l=[]\n\nfor name, model in models.items():\n    model.fit(x_random_train,y_random_train)\n    y_pred = model.predict(x_random_test)\n    acc = accuracy_score(y_random_test, y_pred)\n    print(f\"{name} accuracy: {acc}\")\n    acc_l.append(acc)\nacc_m=pd.DataFrame(acc_l,index=['Logistic Regression','Gradient Boosting','Naive bayes Classifier','Decision Tree','Random Forest'])\nacc_m.rename(columns={ acc_m.columns[0]: \"Random_Sampling\" }, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2024-01-28T11:06:18.276032Z","iopub.execute_input":"2024-01-28T11:06:18.277140Z","iopub.status.idle":"2024-01-28T11:06:18.882501Z","shell.execute_reply.started":"2024-01-28T11:06:18.277099Z","shell.execute_reply":"2024-01-28T11:06:18.880942Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n","output_type":"stream"},{"name":"stdout","text":"Logistic Regression accuracy: 0.8958333333333334\nGradient Boosting accuracy: 0.9895833333333334\nNaive bayes Classifier accuracy: 0.7395833333333334\nDecision Tree accuracy: 0.9583333333333334\nRandom Forest accuracy: 1.0\n","output_type":"stream"}]},{"cell_type":"code","source":"acc_m","metadata":{"execution":{"iopub.status.busy":"2024-01-28T11:07:01.527216Z","iopub.execute_input":"2024-01-28T11:07:01.527719Z","iopub.status.idle":"2024-01-28T11:07:01.540704Z","shell.execute_reply.started":"2024-01-28T11:07:01.527684Z","shell.execute_reply":"2024-01-28T11:07:01.538952Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"                        Random_Sampling\nLogistic Regression            0.895833\nGradient Boosting              0.989583\nNaive bayes Classifier         0.739583\nDecision Tree                  0.958333\nRandom Forest                  1.000000","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Random_Sampling</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Logistic Regression</th>\n      <td>0.895833</td>\n    </tr>\n    <tr>\n      <th>Gradient Boosting</th>\n      <td>0.989583</td>\n    </tr>\n    <tr>\n      <th>Naive bayes Classifier</th>\n      <td>0.739583</td>\n    </tr>\n    <tr>\n      <th>Decision Tree</th>\n      <td>0.958333</td>\n    </tr>\n    <tr>\n      <th>Random Forest</th>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"n = len(cc_balanced)\nn","metadata":{"execution":{"iopub.status.busy":"2024-01-28T11:08:31.116652Z","iopub.execute_input":"2024-01-28T11:08:31.117157Z","iopub.status.idle":"2024-01-28T11:08:31.125683Z","shell.execute_reply.started":"2024-01-28T11:08:31.117117Z","shell.execute_reply":"2024-01-28T11:08:31.124364Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"1526"},"metadata":{}}]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport math\n\n# Read the CSV file into a DataFrame\n\n\n# Separate the feature matrix X and the target variable y\nX = cc_balanced.drop(columns=['Class'])\ny = cc_balanced['Class']\n\n# Determine the number of strata (in this case, we use a binary target variable, so there are two strata)\nnum_strata = 2\n\n# Initialize an empty list to store the stratified samples\nsamples = []\n\n# Loop over each stratum\nfor i in range(num_strata):\n    # Subset the data to include only the observations in the current stratum\n    stratum_data = cc_balanced[cc_balanced['Class'] ==i]\n    \n    # Calculate the sample size for the current stratum\n    stratum_size = len(stratum_data)\n    population_size=len(cc_balanced)\n    p=0.5\n    error=0.05\n    z_score = 1.96  # for a 95% confidence level\n    p = stratum_size / population_size\n    q = 1 - p\n    n = int((z_score**2 * p * q * population_size) / ((z_score**2 * p * q) + (error**2 * (population_size-1))))\n    \n    \n    # If the calculated sample size for the current stratum is greater than the number of observations in the stratum, set the sample size to the number of observations\n    if n > stratum_size:\n        n = stratum_size\n    \n    # Randomly select observations from the current stratum to include in the sample\n    sample_indices = np.random.choice(stratum_data.index, size=n, replace=False)\n    stratum_sample = stratum_data.loc[sample_indices]\n    \n    # Add the current stratum sample to the list of stratified samples\n    samples.append(stratum_sample)\n\n# Combine the stratified samples into a single DataFrame\nstratified_sample = pd.concat(samples)\n\n# Write the stratified sample to a new CSV file\nstratified_sample.to_csv('stratified_dataset.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-01-28T11:12:47.098845Z","iopub.execute_input":"2024-01-28T11:12:47.099343Z","iopub.status.idle":"2024-01-28T11:12:47.168572Z","shell.execute_reply.started":"2024-01-28T11:12:47.099309Z","shell.execute_reply":"2024-01-28T11:12:47.167357Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"stratified_sample","metadata":{"execution":{"iopub.status.busy":"2024-01-28T11:13:42.573805Z","iopub.execute_input":"2024-01-28T11:13:42.574290Z","iopub.status.idle":"2024-01-28T11:13:42.617920Z","shell.execute_reply.started":"2024-01-28T11:13:42.574241Z","shell.execute_reply":"2024-01-28T11:13:42.616695Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"      Time        V1        V2        V3        V4        V5        V6  \\\n311    222  1.081027 -0.139455  0.483881  0.642057 -0.186845  0.538283   \n355    260  1.075229 -0.335339  1.680468  1.192098 -1.270531  0.313498   \n671    509  1.166592  0.124033  0.381999  1.403556 -0.254473 -0.209577   \n475    352 -0.703183  1.210704  0.713731  1.146381  0.196790  0.468060   \n390    284 -0.942623  0.657318  1.191544  1.326497  0.976745 -0.832970   \n...    ...       ...       ...       ...       ...       ...       ...   \n1129   118  1.254914  0.350287  0.302488  0.693114 -0.371470 -1.070256   \n896    406 -2.312227  1.951992 -1.609851  3.997906 -0.522188 -1.426545   \n965    406 -2.312227  1.951992 -1.609851  3.997906 -0.522188 -1.426545   \n791    472 -3.043541 -3.157307  1.088463  2.288644  1.359805 -1.064823   \n1306   472 -3.043541 -3.157307  1.088463  2.288644  1.359805 -1.064823   \n\n            V7        V8        V9  ...       V21       V22       V23  \\\n311  -0.302749  0.315920  0.277328  ... -0.124039 -0.190064  0.057896   \n355  -0.871025  0.235402  1.315261  ...  0.092930  0.815227 -0.054551   \n671  -0.016417  0.054510  0.475647  ... -0.116682 -0.155497 -0.046085   \n475   0.204977  0.627389 -0.296261  ... -0.052939  0.044339 -0.184853   \n390   0.238933  0.163402 -0.584981  ...  0.062165 -0.016076 -0.236314   \n...        ...       ...       ...  ...       ...       ...       ...   \n1129  0.086781 -0.202836  0.035154  ... -0.287592 -0.832682  0.128083   \n896  -2.537387  1.391657 -2.770089  ...  0.517232 -0.035049 -0.465211   \n965  -2.537387  1.391657 -2.770089  ...  0.517232 -0.035049 -0.465211   \n791   0.325574 -0.067794 -0.270953  ...  0.661696  0.435477  1.375966   \n1306  0.325574 -0.067794 -0.270953  ...  0.661696  0.435477  1.375966   \n\n           V24       V25       V26       V27       V28  Amount  Class  \n311  -0.269354  0.253835  0.311886  0.001591 -0.003468   17.24      0  \n355   0.731149  0.431465  0.580189  0.062528  0.025628    5.46      0  \n671   0.061416  0.622487 -0.308575  0.032173  0.013178    6.99      0  \n475  -0.776439  0.021548 -0.166427  0.297892  0.128381   13.99      0  \n390  -0.082802  0.357494 -0.110530  0.080796  0.113264    1.00      0  \n...        ...       ...       ...       ...       ...     ...    ...  \n1129  0.339427  0.215944  0.094704 -0.023354  0.030892    2.69      1  \n896   0.320198  0.044519  0.177840  0.261145 -0.143276    0.00      1  \n965   0.320198  0.044519  0.177840  0.261145 -0.143276    0.00      1  \n791  -0.293803  0.279798 -0.145362 -0.252773  0.035764  529.00      1  \n1306 -0.293803  0.279798 -0.145362 -0.252773  0.035764  529.00      1  \n\n[614 rows x 31 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Time</th>\n      <th>V1</th>\n      <th>V2</th>\n      <th>V3</th>\n      <th>V4</th>\n      <th>V5</th>\n      <th>V6</th>\n      <th>V7</th>\n      <th>V8</th>\n      <th>V9</th>\n      <th>...</th>\n      <th>V21</th>\n      <th>V22</th>\n      <th>V23</th>\n      <th>V24</th>\n      <th>V25</th>\n      <th>V26</th>\n      <th>V27</th>\n      <th>V28</th>\n      <th>Amount</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>311</th>\n      <td>222</td>\n      <td>1.081027</td>\n      <td>-0.139455</td>\n      <td>0.483881</td>\n      <td>0.642057</td>\n      <td>-0.186845</td>\n      <td>0.538283</td>\n      <td>-0.302749</td>\n      <td>0.315920</td>\n      <td>0.277328</td>\n      <td>...</td>\n      <td>-0.124039</td>\n      <td>-0.190064</td>\n      <td>0.057896</td>\n      <td>-0.269354</td>\n      <td>0.253835</td>\n      <td>0.311886</td>\n      <td>0.001591</td>\n      <td>-0.003468</td>\n      <td>17.24</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>355</th>\n      <td>260</td>\n      <td>1.075229</td>\n      <td>-0.335339</td>\n      <td>1.680468</td>\n      <td>1.192098</td>\n      <td>-1.270531</td>\n      <td>0.313498</td>\n      <td>-0.871025</td>\n      <td>0.235402</td>\n      <td>1.315261</td>\n      <td>...</td>\n      <td>0.092930</td>\n      <td>0.815227</td>\n      <td>-0.054551</td>\n      <td>0.731149</td>\n      <td>0.431465</td>\n      <td>0.580189</td>\n      <td>0.062528</td>\n      <td>0.025628</td>\n      <td>5.46</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>671</th>\n      <td>509</td>\n      <td>1.166592</td>\n      <td>0.124033</td>\n      <td>0.381999</td>\n      <td>1.403556</td>\n      <td>-0.254473</td>\n      <td>-0.209577</td>\n      <td>-0.016417</td>\n      <td>0.054510</td>\n      <td>0.475647</td>\n      <td>...</td>\n      <td>-0.116682</td>\n      <td>-0.155497</td>\n      <td>-0.046085</td>\n      <td>0.061416</td>\n      <td>0.622487</td>\n      <td>-0.308575</td>\n      <td>0.032173</td>\n      <td>0.013178</td>\n      <td>6.99</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>475</th>\n      <td>352</td>\n      <td>-0.703183</td>\n      <td>1.210704</td>\n      <td>0.713731</td>\n      <td>1.146381</td>\n      <td>0.196790</td>\n      <td>0.468060</td>\n      <td>0.204977</td>\n      <td>0.627389</td>\n      <td>-0.296261</td>\n      <td>...</td>\n      <td>-0.052939</td>\n      <td>0.044339</td>\n      <td>-0.184853</td>\n      <td>-0.776439</td>\n      <td>0.021548</td>\n      <td>-0.166427</td>\n      <td>0.297892</td>\n      <td>0.128381</td>\n      <td>13.99</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>390</th>\n      <td>284</td>\n      <td>-0.942623</td>\n      <td>0.657318</td>\n      <td>1.191544</td>\n      <td>1.326497</td>\n      <td>0.976745</td>\n      <td>-0.832970</td>\n      <td>0.238933</td>\n      <td>0.163402</td>\n      <td>-0.584981</td>\n      <td>...</td>\n      <td>0.062165</td>\n      <td>-0.016076</td>\n      <td>-0.236314</td>\n      <td>-0.082802</td>\n      <td>0.357494</td>\n      <td>-0.110530</td>\n      <td>0.080796</td>\n      <td>0.113264</td>\n      <td>1.00</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1129</th>\n      <td>118</td>\n      <td>1.254914</td>\n      <td>0.350287</td>\n      <td>0.302488</td>\n      <td>0.693114</td>\n      <td>-0.371470</td>\n      <td>-1.070256</td>\n      <td>0.086781</td>\n      <td>-0.202836</td>\n      <td>0.035154</td>\n      <td>...</td>\n      <td>-0.287592</td>\n      <td>-0.832682</td>\n      <td>0.128083</td>\n      <td>0.339427</td>\n      <td>0.215944</td>\n      <td>0.094704</td>\n      <td>-0.023354</td>\n      <td>0.030892</td>\n      <td>2.69</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>896</th>\n      <td>406</td>\n      <td>-2.312227</td>\n      <td>1.951992</td>\n      <td>-1.609851</td>\n      <td>3.997906</td>\n      <td>-0.522188</td>\n      <td>-1.426545</td>\n      <td>-2.537387</td>\n      <td>1.391657</td>\n      <td>-2.770089</td>\n      <td>...</td>\n      <td>0.517232</td>\n      <td>-0.035049</td>\n      <td>-0.465211</td>\n      <td>0.320198</td>\n      <td>0.044519</td>\n      <td>0.177840</td>\n      <td>0.261145</td>\n      <td>-0.143276</td>\n      <td>0.00</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>965</th>\n      <td>406</td>\n      <td>-2.312227</td>\n      <td>1.951992</td>\n      <td>-1.609851</td>\n      <td>3.997906</td>\n      <td>-0.522188</td>\n      <td>-1.426545</td>\n      <td>-2.537387</td>\n      <td>1.391657</td>\n      <td>-2.770089</td>\n      <td>...</td>\n      <td>0.517232</td>\n      <td>-0.035049</td>\n      <td>-0.465211</td>\n      <td>0.320198</td>\n      <td>0.044519</td>\n      <td>0.177840</td>\n      <td>0.261145</td>\n      <td>-0.143276</td>\n      <td>0.00</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>791</th>\n      <td>472</td>\n      <td>-3.043541</td>\n      <td>-3.157307</td>\n      <td>1.088463</td>\n      <td>2.288644</td>\n      <td>1.359805</td>\n      <td>-1.064823</td>\n      <td>0.325574</td>\n      <td>-0.067794</td>\n      <td>-0.270953</td>\n      <td>...</td>\n      <td>0.661696</td>\n      <td>0.435477</td>\n      <td>1.375966</td>\n      <td>-0.293803</td>\n      <td>0.279798</td>\n      <td>-0.145362</td>\n      <td>-0.252773</td>\n      <td>0.035764</td>\n      <td>529.00</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1306</th>\n      <td>472</td>\n      <td>-3.043541</td>\n      <td>-3.157307</td>\n      <td>1.088463</td>\n      <td>2.288644</td>\n      <td>1.359805</td>\n      <td>-1.064823</td>\n      <td>0.325574</td>\n      <td>-0.067794</td>\n      <td>-0.270953</td>\n      <td>...</td>\n      <td>0.661696</td>\n      <td>0.435477</td>\n      <td>1.375966</td>\n      <td>-0.293803</td>\n      <td>0.279798</td>\n      <td>-0.145362</td>\n      <td>-0.252773</td>\n      <td>0.035764</td>\n      <td>529.00</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>614 rows × 31 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\n\n\n\nfrom sklearn.metrics import accuracy_score\nx_stratified= stratified_sample.drop(columns=['Class'])\ny_stratified = stratified_sample['Class']\nx_stratified_train, x_stratified_test,y_stratified_train, y_stratified_test = train_test_split(x_stratified,y_stratified, random_state=121, \n                                   test_size=0.3, \n                                   shuffle=True)\n\n\n\n# Train and evaluate the models\nmodels = {\n    \"Logistic Regression\": LogisticRegression(),\n    \"Gradient Boosting\": GradientBoostingClassifier(),\n    \"Naive bayes Classifier\":GaussianNB(),\n    \"Decision Tree\": DecisionTreeClassifier(),\n    \"Random Forest\": RandomForestClassifier()\n}\nacc_l3=[]\nfor name, model in models.items():\n    model.fit(x_stratified_train,y_stratified_train)\n    y_pred = model.predict(x_stratified_test)\n    acc = accuracy_score(y_stratified_test, y_pred)\n    print(f\"{name} accuracy: {acc}\")\n    acc_l3.append(acc)\nacc_m['Stratified_Sampling']=acc_l3","metadata":{"execution":{"iopub.status.busy":"2024-01-28T11:14:57.448595Z","iopub.execute_input":"2024-01-28T11:14:57.449150Z","iopub.status.idle":"2024-01-28T11:14:58.302566Z","shell.execute_reply.started":"2024-01-28T11:14:57.449108Z","shell.execute_reply":"2024-01-28T11:14:58.301380Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n","output_type":"stream"},{"name":"stdout","text":"Logistic Regression accuracy: 0.9243243243243243\nGradient Boosting accuracy: 0.9945945945945946\nNaive bayes Classifier accuracy: 0.6810810810810811\nDecision Tree accuracy: 0.9783783783783784\nRandom Forest accuracy: 1.0\n","output_type":"stream"}]},{"cell_type":"code","source":"acc_m","metadata":{"execution":{"iopub.status.busy":"2024-01-28T11:15:07.950686Z","iopub.execute_input":"2024-01-28T11:15:07.951192Z","iopub.status.idle":"2024-01-28T11:15:07.966291Z","shell.execute_reply.started":"2024-01-28T11:15:07.951146Z","shell.execute_reply":"2024-01-28T11:15:07.964869Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"                        Random_Sampling  Stratified_Sampling\nLogistic Regression            0.895833             0.924324\nGradient Boosting              0.989583             0.994595\nNaive bayes Classifier         0.739583             0.681081\nDecision Tree                  0.958333             0.978378\nRandom Forest                  1.000000             1.000000","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Random_Sampling</th>\n      <th>Stratified_Sampling</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Logistic Regression</th>\n      <td>0.895833</td>\n      <td>0.924324</td>\n    </tr>\n    <tr>\n      <th>Gradient Boosting</th>\n      <td>0.989583</td>\n      <td>0.994595</td>\n    </tr>\n    <tr>\n      <th>Naive bayes Classifier</th>\n      <td>0.739583</td>\n      <td>0.681081</td>\n    </tr>\n    <tr>\n      <th>Decision Tree</th>\n      <td>0.958333</td>\n      <td>0.978378</td>\n    </tr>\n    <tr>\n      <th>Random Forest</th>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Set the sampling interval \"k\" as the square root of the number of rows in the dataset\nk = int(math.sqrt(n))\ncc_systematic_sample = cc_balanced.iloc[::k]","metadata":{"execution":{"iopub.status.busy":"2024-01-28T11:15:46.812502Z","iopub.execute_input":"2024-01-28T11:15:46.812935Z","iopub.status.idle":"2024-01-28T11:15:46.820752Z","shell.execute_reply.started":"2024-01-28T11:15:46.812903Z","shell.execute_reply":"2024-01-28T11:15:46.819152Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"cc_systematic_sample","metadata":{"execution":{"iopub.status.busy":"2024-01-28T11:15:57.824708Z","iopub.execute_input":"2024-01-28T11:15:57.825121Z","iopub.status.idle":"2024-01-28T11:15:57.870912Z","shell.execute_reply.started":"2024-01-28T11:15:57.825092Z","shell.execute_reply":"2024-01-28T11:15:57.869667Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"      Time        V1        V2        V3        V4        V5        V6  \\\n0        0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388   \n17      13 -0.436905  0.918966  0.924591 -0.727219  0.915679 -0.127867   \n34      26 -0.535388  0.865268  1.351076  0.147575  0.433680  0.086983   \n51      36 -1.004929 -0.985978 -0.038039  3.710061 -6.631951  5.122103   \n68      44  0.927060 -0.323684  0.387585  0.544474  0.246787  1.650358   \n...    ...       ...       ...       ...       ...       ...       ...   \n1445   472 -3.043541 -3.157307  1.088463  2.288644  1.359805 -1.064823   \n1462   164  0.073497  0.551033  0.451890  0.114964  0.822947  0.251480   \n1479   529 -2.000567 -2.495484  2.467149  1.140053  2.462010  0.594262   \n1496   118  1.254914  0.350287  0.302488  0.693114 -0.371470 -1.070256   \n1513   472 -3.043541 -3.157307  1.088463  2.288644  1.359805 -1.064823   \n\n            V7        V8        V9  ...       V21       V22       V23  \\\n0     0.239599  0.098698  0.363787  ... -0.018307  0.277838 -0.110474   \n17    0.707642  0.087962 -0.665271  ... -0.194796 -0.672638 -0.156858   \n34    0.693039  0.179742 -0.285642  ...  0.049526  0.206537 -0.187108   \n51    4.371691 -2.006868 -0.278736  ...  1.393406 -0.381671  0.969719   \n68   -0.427576  0.615371  0.226278  ... -0.040513  0.079359  0.096632   \n...        ...       ...       ...  ...       ...       ...       ...   \n1445  0.325574 -0.067794 -0.270953  ...  0.661696  0.435477  1.375966   \n1462  0.296319  0.139497 -0.123050  ... -0.128758 -0.381932  0.151012   \n1479 -2.110183  0.788347  0.958809  ...  0.422452  1.195394  0.297836   \n1496  0.086781 -0.202836  0.035154  ... -0.287592 -0.832682  0.128083   \n1513  0.325574 -0.067794 -0.270953  ...  0.661696  0.435477  1.375966   \n\n           V24       V25       V26       V27       V28   Amount  Class  \n0     0.066928  0.128539 -0.189115  0.133558 -0.021053   149.62      0  \n17   -0.888386 -0.342413 -0.049027  0.079692  0.131024     0.89      0  \n34    0.000753  0.098117 -0.553471 -0.078306  0.025427     1.77      0  \n51    0.019445  0.570923  0.333278  0.857373 -0.075538  1402.95      0  \n68   -0.992569  0.085096  0.377447  0.036096 -0.005960    45.71      0  \n...        ...       ...       ...       ...       ...      ...    ...  \n1445 -0.293803  0.279798 -0.145362 -0.252773  0.035764   529.00      1  \n1462 -1.363967 -1.389079  0.075412  0.231750  0.230171     0.99      1  \n1479 -0.857105 -0.219322  0.861019 -0.124622 -0.171060     1.50      1  \n1496  0.339427  0.215944  0.094704 -0.023354  0.030892     2.69      1  \n1513 -0.293803  0.279798 -0.145362 -0.252773  0.035764   529.00      1  \n\n[90 rows x 31 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Time</th>\n      <th>V1</th>\n      <th>V2</th>\n      <th>V3</th>\n      <th>V4</th>\n      <th>V5</th>\n      <th>V6</th>\n      <th>V7</th>\n      <th>V8</th>\n      <th>V9</th>\n      <th>...</th>\n      <th>V21</th>\n      <th>V22</th>\n      <th>V23</th>\n      <th>V24</th>\n      <th>V25</th>\n      <th>V26</th>\n      <th>V27</th>\n      <th>V28</th>\n      <th>Amount</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>-1.359807</td>\n      <td>-0.072781</td>\n      <td>2.536347</td>\n      <td>1.378155</td>\n      <td>-0.338321</td>\n      <td>0.462388</td>\n      <td>0.239599</td>\n      <td>0.098698</td>\n      <td>0.363787</td>\n      <td>...</td>\n      <td>-0.018307</td>\n      <td>0.277838</td>\n      <td>-0.110474</td>\n      <td>0.066928</td>\n      <td>0.128539</td>\n      <td>-0.189115</td>\n      <td>0.133558</td>\n      <td>-0.021053</td>\n      <td>149.62</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>13</td>\n      <td>-0.436905</td>\n      <td>0.918966</td>\n      <td>0.924591</td>\n      <td>-0.727219</td>\n      <td>0.915679</td>\n      <td>-0.127867</td>\n      <td>0.707642</td>\n      <td>0.087962</td>\n      <td>-0.665271</td>\n      <td>...</td>\n      <td>-0.194796</td>\n      <td>-0.672638</td>\n      <td>-0.156858</td>\n      <td>-0.888386</td>\n      <td>-0.342413</td>\n      <td>-0.049027</td>\n      <td>0.079692</td>\n      <td>0.131024</td>\n      <td>0.89</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>26</td>\n      <td>-0.535388</td>\n      <td>0.865268</td>\n      <td>1.351076</td>\n      <td>0.147575</td>\n      <td>0.433680</td>\n      <td>0.086983</td>\n      <td>0.693039</td>\n      <td>0.179742</td>\n      <td>-0.285642</td>\n      <td>...</td>\n      <td>0.049526</td>\n      <td>0.206537</td>\n      <td>-0.187108</td>\n      <td>0.000753</td>\n      <td>0.098117</td>\n      <td>-0.553471</td>\n      <td>-0.078306</td>\n      <td>0.025427</td>\n      <td>1.77</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>51</th>\n      <td>36</td>\n      <td>-1.004929</td>\n      <td>-0.985978</td>\n      <td>-0.038039</td>\n      <td>3.710061</td>\n      <td>-6.631951</td>\n      <td>5.122103</td>\n      <td>4.371691</td>\n      <td>-2.006868</td>\n      <td>-0.278736</td>\n      <td>...</td>\n      <td>1.393406</td>\n      <td>-0.381671</td>\n      <td>0.969719</td>\n      <td>0.019445</td>\n      <td>0.570923</td>\n      <td>0.333278</td>\n      <td>0.857373</td>\n      <td>-0.075538</td>\n      <td>1402.95</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>68</th>\n      <td>44</td>\n      <td>0.927060</td>\n      <td>-0.323684</td>\n      <td>0.387585</td>\n      <td>0.544474</td>\n      <td>0.246787</td>\n      <td>1.650358</td>\n      <td>-0.427576</td>\n      <td>0.615371</td>\n      <td>0.226278</td>\n      <td>...</td>\n      <td>-0.040513</td>\n      <td>0.079359</td>\n      <td>0.096632</td>\n      <td>-0.992569</td>\n      <td>0.085096</td>\n      <td>0.377447</td>\n      <td>0.036096</td>\n      <td>-0.005960</td>\n      <td>45.71</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1445</th>\n      <td>472</td>\n      <td>-3.043541</td>\n      <td>-3.157307</td>\n      <td>1.088463</td>\n      <td>2.288644</td>\n      <td>1.359805</td>\n      <td>-1.064823</td>\n      <td>0.325574</td>\n      <td>-0.067794</td>\n      <td>-0.270953</td>\n      <td>...</td>\n      <td>0.661696</td>\n      <td>0.435477</td>\n      <td>1.375966</td>\n      <td>-0.293803</td>\n      <td>0.279798</td>\n      <td>-0.145362</td>\n      <td>-0.252773</td>\n      <td>0.035764</td>\n      <td>529.00</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1462</th>\n      <td>164</td>\n      <td>0.073497</td>\n      <td>0.551033</td>\n      <td>0.451890</td>\n      <td>0.114964</td>\n      <td>0.822947</td>\n      <td>0.251480</td>\n      <td>0.296319</td>\n      <td>0.139497</td>\n      <td>-0.123050</td>\n      <td>...</td>\n      <td>-0.128758</td>\n      <td>-0.381932</td>\n      <td>0.151012</td>\n      <td>-1.363967</td>\n      <td>-1.389079</td>\n      <td>0.075412</td>\n      <td>0.231750</td>\n      <td>0.230171</td>\n      <td>0.99</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1479</th>\n      <td>529</td>\n      <td>-2.000567</td>\n      <td>-2.495484</td>\n      <td>2.467149</td>\n      <td>1.140053</td>\n      <td>2.462010</td>\n      <td>0.594262</td>\n      <td>-2.110183</td>\n      <td>0.788347</td>\n      <td>0.958809</td>\n      <td>...</td>\n      <td>0.422452</td>\n      <td>1.195394</td>\n      <td>0.297836</td>\n      <td>-0.857105</td>\n      <td>-0.219322</td>\n      <td>0.861019</td>\n      <td>-0.124622</td>\n      <td>-0.171060</td>\n      <td>1.50</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1496</th>\n      <td>118</td>\n      <td>1.254914</td>\n      <td>0.350287</td>\n      <td>0.302488</td>\n      <td>0.693114</td>\n      <td>-0.371470</td>\n      <td>-1.070256</td>\n      <td>0.086781</td>\n      <td>-0.202836</td>\n      <td>0.035154</td>\n      <td>...</td>\n      <td>-0.287592</td>\n      <td>-0.832682</td>\n      <td>0.128083</td>\n      <td>0.339427</td>\n      <td>0.215944</td>\n      <td>0.094704</td>\n      <td>-0.023354</td>\n      <td>0.030892</td>\n      <td>2.69</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1513</th>\n      <td>472</td>\n      <td>-3.043541</td>\n      <td>-3.157307</td>\n      <td>1.088463</td>\n      <td>2.288644</td>\n      <td>1.359805</td>\n      <td>-1.064823</td>\n      <td>0.325574</td>\n      <td>-0.067794</td>\n      <td>-0.270953</td>\n      <td>...</td>\n      <td>0.661696</td>\n      <td>0.435477</td>\n      <td>1.375966</td>\n      <td>-0.293803</td>\n      <td>0.279798</td>\n      <td>-0.145362</td>\n      <td>-0.252773</td>\n      <td>0.035764</td>\n      <td>529.00</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>90 rows × 31 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"#Now we will aplly 5 different models to our systematic sample\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\n\n\n\nfrom sklearn.metrics import accuracy_score\nx_systematic= cc_systematic_sample.drop(columns=['Class'])\ny_systematic = cc_systematic_sample['Class']\nx_systematic_train, x_systematic_test,y_systematic_train, y_systematic_test = train_test_split(x_systematic,y_systematic, random_state=121, \n                                   test_size=0.3, \n                                   shuffle=True)\n\n\n\n# Train and evaluate the models\nmodels = {\n    \"Logistic Regression\": LogisticRegression(),\n    \"Gradient Boosting\": GradientBoostingClassifier(),\n    \"Naive bayes Classifier\":GaussianNB(),\n    \"Decision Tree\": DecisionTreeClassifier(),\n    \"Random Forest\": RandomForestClassifier()  \n}\nacc_l2=[]\nfor name, model in models.items():\n    model.fit(x_systematic_train,y_systematic_train)\n    y_pred = model.predict(x_systematic_test)\n    acc = accuracy_score(y_systematic_test, y_pred)\n    print(f\"{name} accuracy: {acc}\")\n    acc_l2.append(acc)\nacc_m['Systematic_Sampling']=acc_l2","metadata":{"execution":{"iopub.status.busy":"2024-01-28T11:16:32.129068Z","iopub.execute_input":"2024-01-28T11:16:32.130349Z","iopub.status.idle":"2024-01-28T11:16:32.535936Z","shell.execute_reply.started":"2024-01-28T11:16:32.130291Z","shell.execute_reply":"2024-01-28T11:16:32.534593Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n","output_type":"stream"},{"name":"stdout","text":"Logistic Regression accuracy: 0.9259259259259259\nGradient Boosting accuracy: 0.9259259259259259\nNaive bayes Classifier accuracy: 0.7407407407407407\nDecision Tree accuracy: 0.9259259259259259\nRandom Forest accuracy: 0.9629629629629629\n","output_type":"stream"}]},{"cell_type":"code","source":"acc_m","metadata":{"execution":{"iopub.status.busy":"2024-01-28T11:16:39.593347Z","iopub.execute_input":"2024-01-28T11:16:39.593864Z","iopub.status.idle":"2024-01-28T11:16:39.608958Z","shell.execute_reply.started":"2024-01-28T11:16:39.593830Z","shell.execute_reply":"2024-01-28T11:16:39.606950Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"                        Random_Sampling  Stratified_Sampling  \\\nLogistic Regression            0.895833             0.924324   \nGradient Boosting              0.989583             0.994595   \nNaive bayes Classifier         0.739583             0.681081   \nDecision Tree                  0.958333             0.978378   \nRandom Forest                  1.000000             1.000000   \n\n                        Systematic_Sampling  \nLogistic Regression                0.925926  \nGradient Boosting                  0.925926  \nNaive bayes Classifier             0.740741  \nDecision Tree                      0.925926  \nRandom Forest                      0.962963  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Random_Sampling</th>\n      <th>Stratified_Sampling</th>\n      <th>Systematic_Sampling</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Logistic Regression</th>\n      <td>0.895833</td>\n      <td>0.924324</td>\n      <td>0.925926</td>\n    </tr>\n    <tr>\n      <th>Gradient Boosting</th>\n      <td>0.989583</td>\n      <td>0.994595</td>\n      <td>0.925926</td>\n    </tr>\n    <tr>\n      <th>Naive bayes Classifier</th>\n      <td>0.739583</td>\n      <td>0.681081</td>\n      <td>0.740741</td>\n    </tr>\n    <tr>\n      <th>Decision Tree</th>\n      <td>0.958333</td>\n      <td>0.978378</td>\n      <td>0.925926</td>\n    </tr>\n    <tr>\n      <th>Random Forest</th>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.962963</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\nX = cc.drop(columns=['Class'])\ny = cc['Class']\nsmote = SMOTE()\n\n# fit predictor and target variable\nx_smote, y_smote = smote.fit_resample(x, y)\nsmote_random = pd.concat((pd.DataFrame(x_smote),pd.DataFrame(y_smote)),axis=1)\nnp.random.seed(0)\n\n#using the formula\nsample_size=int((pow(1.96,2)*0.5*0.5)/0.0025)\n\nsmote_random_sample = smote_random.sample(n=sample_size, random_state=0)","metadata":{"execution":{"iopub.status.busy":"2024-01-28T11:17:04.483606Z","iopub.execute_input":"2024-01-28T11:17:04.484082Z","iopub.status.idle":"2024-01-28T11:17:04.569288Z","shell.execute_reply.started":"2024-01-28T11:17:04.484048Z","shell.execute_reply":"2024-01-28T11:17:04.568324Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"smote_random_sample","metadata":{"execution":{"iopub.status.busy":"2024-01-28T11:17:51.650854Z","iopub.execute_input":"2024-01-28T11:17:51.651436Z","iopub.status.idle":"2024-01-28T11:17:51.694526Z","shell.execute_reply.started":"2024-01-28T11:17:51.651394Z","shell.execute_reply":"2024-01-28T11:17:51.693481Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"      Time        V1        V2        V3        V4        V5        V6  \\\n1361   424 -1.976060  1.574621 -0.795997  3.071299 -0.160657 -1.298907   \n511    377  1.166919  0.027049  0.513875  0.860965 -0.519452 -0.681147   \n9        9 -0.338262  1.119593  1.044367 -0.222187  0.499361 -0.246761   \n393    284 -0.810756  0.654499  2.217257  0.104341 -0.286801  0.117833   \n471    346  1.077079  0.284980  0.007731  1.657073  0.052020  0.446389   \n...    ...       ...       ...       ...       ...       ...       ...   \n829     69  0.719346  0.386515  0.287067  0.307380  0.382358  0.058688   \n530    394  1.293053  0.457969 -1.940450  0.173149  2.609570  3.014117   \n1363   336 -0.908943 -0.892036  1.406475  0.600527  1.599336  0.413849   \n795    492 -0.710404  0.394862  1.598299  0.233261  0.834512 -0.917583   \n1370    81  0.633434  0.408399  0.308992  0.281785  0.440966  0.084334   \n\n            V7        V8        V9  ...       V21       V22       V23  \\\n1361 -1.707645  1.015634 -2.131833  ...  0.407727  0.041801 -0.414877   \n511   0.074992 -0.187776  0.345399  ... -0.202750 -0.441391 -0.025782   \n9     0.651583  0.069539 -0.736727  ... -0.246914 -0.633753 -0.120794   \n393   0.287552 -0.736461  0.699092  ...  0.938194  0.571651 -0.101609   \n471  -0.407036  0.355704  0.626039  ... -0.174337 -0.174161 -0.153375   \n...        ...       ...       ...  ...       ...       ...       ...   \n829   0.079687  0.108084 -0.199496  ... -0.184785 -0.530198  0.122296   \n530  -0.269415  0.754420 -0.221009  ... -0.121126 -0.427753 -0.159336   \n1363 -0.843589  0.446843  0.389403  ...  0.132338  0.365213  0.220559   \n795   0.800864 -0.161939 -0.126524  ...  0.031177  0.171626 -0.219561   \n1370  0.108504  0.112262 -0.189327  ... -0.177332 -0.510476  0.126116   \n\n           V24       V25       V26       V27       V28     Amount  Class  \n1361  0.336030  0.128697  0.024420  0.172353 -0.139023   0.242870      1  \n511   0.452607  0.467223  0.262577 -0.023834  0.020521  40.830000      0  \n9    -0.385050 -0.069733  0.094199  0.246219  0.083076   3.680000      0  \n393   0.363928 -0.170947 -0.471524  0.058958 -0.079157  30.300000      0  \n471  -0.466331  0.611001 -0.252871  0.090375  0.054820  10.990000      0  \n...        ...       ...       ...       ...       ...        ...    ...  \n829  -0.772541 -0.490351  0.104566  0.092727  0.105751   1.971744      1  \n530   0.857135  0.850055 -0.311685  0.037536  0.050618   1.000000      0  \n1363 -1.123877 -0.834990  0.447537  0.062944  0.040116   1.231576      1  \n795   0.381176  0.374187 -0.399254 -0.096256 -0.110159   1.028881      1  \n1370 -0.851214 -0.609901  0.100688  0.111220  0.122302   1.841151      1  \n\n[384 rows x 31 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Time</th>\n      <th>V1</th>\n      <th>V2</th>\n      <th>V3</th>\n      <th>V4</th>\n      <th>V5</th>\n      <th>V6</th>\n      <th>V7</th>\n      <th>V8</th>\n      <th>V9</th>\n      <th>...</th>\n      <th>V21</th>\n      <th>V22</th>\n      <th>V23</th>\n      <th>V24</th>\n      <th>V25</th>\n      <th>V26</th>\n      <th>V27</th>\n      <th>V28</th>\n      <th>Amount</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1361</th>\n      <td>424</td>\n      <td>-1.976060</td>\n      <td>1.574621</td>\n      <td>-0.795997</td>\n      <td>3.071299</td>\n      <td>-0.160657</td>\n      <td>-1.298907</td>\n      <td>-1.707645</td>\n      <td>1.015634</td>\n      <td>-2.131833</td>\n      <td>...</td>\n      <td>0.407727</td>\n      <td>0.041801</td>\n      <td>-0.414877</td>\n      <td>0.336030</td>\n      <td>0.128697</td>\n      <td>0.024420</td>\n      <td>0.172353</td>\n      <td>-0.139023</td>\n      <td>0.242870</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>511</th>\n      <td>377</td>\n      <td>1.166919</td>\n      <td>0.027049</td>\n      <td>0.513875</td>\n      <td>0.860965</td>\n      <td>-0.519452</td>\n      <td>-0.681147</td>\n      <td>0.074992</td>\n      <td>-0.187776</td>\n      <td>0.345399</td>\n      <td>...</td>\n      <td>-0.202750</td>\n      <td>-0.441391</td>\n      <td>-0.025782</td>\n      <td>0.452607</td>\n      <td>0.467223</td>\n      <td>0.262577</td>\n      <td>-0.023834</td>\n      <td>0.020521</td>\n      <td>40.830000</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>9</td>\n      <td>-0.338262</td>\n      <td>1.119593</td>\n      <td>1.044367</td>\n      <td>-0.222187</td>\n      <td>0.499361</td>\n      <td>-0.246761</td>\n      <td>0.651583</td>\n      <td>0.069539</td>\n      <td>-0.736727</td>\n      <td>...</td>\n      <td>-0.246914</td>\n      <td>-0.633753</td>\n      <td>-0.120794</td>\n      <td>-0.385050</td>\n      <td>-0.069733</td>\n      <td>0.094199</td>\n      <td>0.246219</td>\n      <td>0.083076</td>\n      <td>3.680000</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>393</th>\n      <td>284</td>\n      <td>-0.810756</td>\n      <td>0.654499</td>\n      <td>2.217257</td>\n      <td>0.104341</td>\n      <td>-0.286801</td>\n      <td>0.117833</td>\n      <td>0.287552</td>\n      <td>-0.736461</td>\n      <td>0.699092</td>\n      <td>...</td>\n      <td>0.938194</td>\n      <td>0.571651</td>\n      <td>-0.101609</td>\n      <td>0.363928</td>\n      <td>-0.170947</td>\n      <td>-0.471524</td>\n      <td>0.058958</td>\n      <td>-0.079157</td>\n      <td>30.300000</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>471</th>\n      <td>346</td>\n      <td>1.077079</td>\n      <td>0.284980</td>\n      <td>0.007731</td>\n      <td>1.657073</td>\n      <td>0.052020</td>\n      <td>0.446389</td>\n      <td>-0.407036</td>\n      <td>0.355704</td>\n      <td>0.626039</td>\n      <td>...</td>\n      <td>-0.174337</td>\n      <td>-0.174161</td>\n      <td>-0.153375</td>\n      <td>-0.466331</td>\n      <td>0.611001</td>\n      <td>-0.252871</td>\n      <td>0.090375</td>\n      <td>0.054820</td>\n      <td>10.990000</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>829</th>\n      <td>69</td>\n      <td>0.719346</td>\n      <td>0.386515</td>\n      <td>0.287067</td>\n      <td>0.307380</td>\n      <td>0.382358</td>\n      <td>0.058688</td>\n      <td>0.079687</td>\n      <td>0.108084</td>\n      <td>-0.199496</td>\n      <td>...</td>\n      <td>-0.184785</td>\n      <td>-0.530198</td>\n      <td>0.122296</td>\n      <td>-0.772541</td>\n      <td>-0.490351</td>\n      <td>0.104566</td>\n      <td>0.092727</td>\n      <td>0.105751</td>\n      <td>1.971744</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>530</th>\n      <td>394</td>\n      <td>1.293053</td>\n      <td>0.457969</td>\n      <td>-1.940450</td>\n      <td>0.173149</td>\n      <td>2.609570</td>\n      <td>3.014117</td>\n      <td>-0.269415</td>\n      <td>0.754420</td>\n      <td>-0.221009</td>\n      <td>...</td>\n      <td>-0.121126</td>\n      <td>-0.427753</td>\n      <td>-0.159336</td>\n      <td>0.857135</td>\n      <td>0.850055</td>\n      <td>-0.311685</td>\n      <td>0.037536</td>\n      <td>0.050618</td>\n      <td>1.000000</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1363</th>\n      <td>336</td>\n      <td>-0.908943</td>\n      <td>-0.892036</td>\n      <td>1.406475</td>\n      <td>0.600527</td>\n      <td>1.599336</td>\n      <td>0.413849</td>\n      <td>-0.843589</td>\n      <td>0.446843</td>\n      <td>0.389403</td>\n      <td>...</td>\n      <td>0.132338</td>\n      <td>0.365213</td>\n      <td>0.220559</td>\n      <td>-1.123877</td>\n      <td>-0.834990</td>\n      <td>0.447537</td>\n      <td>0.062944</td>\n      <td>0.040116</td>\n      <td>1.231576</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>795</th>\n      <td>492</td>\n      <td>-0.710404</td>\n      <td>0.394862</td>\n      <td>1.598299</td>\n      <td>0.233261</td>\n      <td>0.834512</td>\n      <td>-0.917583</td>\n      <td>0.800864</td>\n      <td>-0.161939</td>\n      <td>-0.126524</td>\n      <td>...</td>\n      <td>0.031177</td>\n      <td>0.171626</td>\n      <td>-0.219561</td>\n      <td>0.381176</td>\n      <td>0.374187</td>\n      <td>-0.399254</td>\n      <td>-0.096256</td>\n      <td>-0.110159</td>\n      <td>1.028881</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1370</th>\n      <td>81</td>\n      <td>0.633434</td>\n      <td>0.408399</td>\n      <td>0.308992</td>\n      <td>0.281785</td>\n      <td>0.440966</td>\n      <td>0.084334</td>\n      <td>0.108504</td>\n      <td>0.112262</td>\n      <td>-0.189327</td>\n      <td>...</td>\n      <td>-0.177332</td>\n      <td>-0.510476</td>\n      <td>0.126116</td>\n      <td>-0.851214</td>\n      <td>-0.609901</td>\n      <td>0.100688</td>\n      <td>0.111220</td>\n      <td>0.122302</td>\n      <td>1.841151</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>384 rows × 31 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\n\n\n\nfrom sklearn.metrics import accuracy_score\nx_smote= smote_random_sample.drop(columns=['Class'])\ny_smote = smote_random_sample['Class']\nx_smote_train, x_smote_test,y_smote_train, y_smote_test = train_test_split(x_smote,y_smote, random_state=121, \n                                   test_size=0.3, \n                                   shuffle=True)\n\n\n\n# Train and evaluate the models\nmodels = {\n    \"Logistic Regression\": LogisticRegression(),\n     \"Gradient Boosting\": GradientBoostingClassifier(),\n    \"Naive bayes Classifier\":GaussianNB(),\n    \"Decision Tree\": DecisionTreeClassifier(),\n    \"Random Forest\": RandomForestClassifier()  \n}\nacc_l5=[]\nfor name, model in models.items():\n    model.fit(x_smote_train,y_smote_train)\n    y_pred = model.predict(x_smote_test)\n    acc = accuracy_score(y_smote_test, y_pred)\n    print(f\"{name} accuracy: {acc}\")\n    acc_l5.append(acc)\nacc_m['SMOTE']=acc_l5","metadata":{"execution":{"iopub.status.busy":"2024-01-28T11:18:15.426145Z","iopub.execute_input":"2024-01-28T11:18:15.427755Z","iopub.status.idle":"2024-01-28T11:18:16.147937Z","shell.execute_reply.started":"2024-01-28T11:18:15.427708Z","shell.execute_reply":"2024-01-28T11:18:16.146419Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n","output_type":"stream"},{"name":"stdout","text":"Logistic Regression accuracy: 0.896551724137931\nGradient Boosting accuracy: 0.9741379310344828\nNaive bayes Classifier accuracy: 0.8362068965517241\nDecision Tree accuracy: 0.9137931034482759\nRandom Forest accuracy: 0.9827586206896551\n","output_type":"stream"}]},{"cell_type":"code","source":"acc_m","metadata":{"execution":{"iopub.status.busy":"2024-01-28T11:18:20.407750Z","iopub.execute_input":"2024-01-28T11:18:20.408301Z","iopub.status.idle":"2024-01-28T11:18:20.424932Z","shell.execute_reply.started":"2024-01-28T11:18:20.408237Z","shell.execute_reply":"2024-01-28T11:18:20.423837Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"                        Random_Sampling  Stratified_Sampling  \\\nLogistic Regression            0.895833             0.924324   \nGradient Boosting              0.989583             0.994595   \nNaive bayes Classifier         0.739583             0.681081   \nDecision Tree                  0.958333             0.978378   \nRandom Forest                  1.000000             1.000000   \n\n                        Systematic_Sampling     SMOTE  \nLogistic Regression                0.925926  0.896552  \nGradient Boosting                  0.925926  0.974138  \nNaive bayes Classifier             0.740741  0.836207  \nDecision Tree                      0.925926  0.913793  \nRandom Forest                      0.962963  0.982759  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Random_Sampling</th>\n      <th>Stratified_Sampling</th>\n      <th>Systematic_Sampling</th>\n      <th>SMOTE</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Logistic Regression</th>\n      <td>0.895833</td>\n      <td>0.924324</td>\n      <td>0.925926</td>\n      <td>0.896552</td>\n    </tr>\n    <tr>\n      <th>Gradient Boosting</th>\n      <td>0.989583</td>\n      <td>0.994595</td>\n      <td>0.925926</td>\n      <td>0.974138</td>\n    </tr>\n    <tr>\n      <th>Naive bayes Classifier</th>\n      <td>0.739583</td>\n      <td>0.681081</td>\n      <td>0.740741</td>\n      <td>0.836207</td>\n    </tr>\n    <tr>\n      <th>Decision Tree</th>\n      <td>0.958333</td>\n      <td>0.978378</td>\n      <td>0.925926</td>\n      <td>0.913793</td>\n    </tr>\n    <tr>\n      <th>Random Forest</th>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.962963</td>\n      <td>0.982759</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"np.random.seed(0)\n\nsample_size=500\ncon_sample = cc_balanced.sample(n=sample_size, random_state=91)","metadata":{"execution":{"iopub.status.busy":"2024-01-28T11:18:42.725413Z","iopub.execute_input":"2024-01-28T11:18:42.725956Z","iopub.status.idle":"2024-01-28T11:18:42.736490Z","shell.execute_reply.started":"2024-01-28T11:18:42.725918Z","shell.execute_reply":"2024-01-28T11:18:42.734618Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"len(con_sample)","metadata":{"execution":{"iopub.status.busy":"2024-01-28T11:18:51.883060Z","iopub.execute_input":"2024-01-28T11:18:51.883525Z","iopub.status.idle":"2024-01-28T11:18:51.891371Z","shell.execute_reply.started":"2024-01-28T11:18:51.883491Z","shell.execute_reply":"2024-01-28T11:18:51.890102Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"500"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\n\n\n\nfrom sklearn.metrics import accuracy_score\nx_con= con_sample.drop(columns=['Class'])\ny_con = con_sample['Class']\nx_con_train, x_con_test,y_con_train, y_con_test = train_test_split(x_con,y_con, random_state=121, \n                                   test_size=0.3, \n                                   shuffle=True)\n\n\n\n# Train and evaluate the models\nmodels = {\n    \"Logistic Regression\": LogisticRegression(),\n     \"Gradient Boosting\": GradientBoostingClassifier(),\n    \"Naive bayes Classifier\":GaussianNB(),\n    \"Decision Tree\": DecisionTreeClassifier(),\n    \"Random Forest\": RandomForestClassifier()  \n}\nacc_l4=[]\nfor name, model in models.items():\n    model.fit(x_con_train,y_con_train)\n    y_pred = model.predict(x_con_test)\n    acc = accuracy_score(y_con_test, y_pred)\n    print(f\"{name} accuracy: {acc}\")\n    acc_l4.append(acc)\nacc_m['Convinience_Sampling']=acc_l4","metadata":{"execution":{"iopub.status.busy":"2024-01-28T11:19:33.501907Z","iopub.execute_input":"2024-01-28T11:19:33.502361Z","iopub.status.idle":"2024-01-28T11:19:34.287503Z","shell.execute_reply.started":"2024-01-28T11:19:33.502327Z","shell.execute_reply":"2024-01-28T11:19:34.286283Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n","output_type":"stream"},{"name":"stdout","text":"Logistic Regression accuracy: 0.8733333333333333\nGradient Boosting accuracy: 0.9866666666666667\nNaive bayes Classifier accuracy: 0.8\nDecision Tree accuracy: 0.98\nRandom Forest accuracy: 0.9933333333333333\n","output_type":"stream"}]},{"cell_type":"code","source":"Final_matrix=acc_m\nFinal_matrix","metadata":{"execution":{"iopub.status.busy":"2024-01-28T11:19:54.170603Z","iopub.execute_input":"2024-01-28T11:19:54.171128Z","iopub.status.idle":"2024-01-28T11:19:54.189660Z","shell.execute_reply.started":"2024-01-28T11:19:54.171087Z","shell.execute_reply":"2024-01-28T11:19:54.188336Z"},"trusted":true},"execution_count":27,"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"                        Random_Sampling  Stratified_Sampling  \\\nLogistic Regression            0.895833             0.924324   \nGradient Boosting              0.989583             0.994595   \nNaive bayes Classifier         0.739583             0.681081   \nDecision Tree                  0.958333             0.978378   \nRandom Forest                  1.000000             1.000000   \n\n                        Systematic_Sampling     SMOTE  Convinience_Sampling  \nLogistic Regression                0.925926  0.896552              0.873333  \nGradient Boosting                  0.925926  0.974138              0.986667  \nNaive bayes Classifier             0.740741  0.836207              0.800000  \nDecision Tree                      0.925926  0.913793              0.980000  \nRandom Forest                      0.962963  0.982759              0.993333  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Random_Sampling</th>\n      <th>Stratified_Sampling</th>\n      <th>Systematic_Sampling</th>\n      <th>SMOTE</th>\n      <th>Convinience_Sampling</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Logistic Regression</th>\n      <td>0.895833</td>\n      <td>0.924324</td>\n      <td>0.925926</td>\n      <td>0.896552</td>\n      <td>0.873333</td>\n    </tr>\n    <tr>\n      <th>Gradient Boosting</th>\n      <td>0.989583</td>\n      <td>0.994595</td>\n      <td>0.925926</td>\n      <td>0.974138</td>\n      <td>0.986667</td>\n    </tr>\n    <tr>\n      <th>Naive bayes Classifier</th>\n      <td>0.739583</td>\n      <td>0.681081</td>\n      <td>0.740741</td>\n      <td>0.836207</td>\n      <td>0.800000</td>\n    </tr>\n    <tr>\n      <th>Decision Tree</th>\n      <td>0.958333</td>\n      <td>0.978378</td>\n      <td>0.925926</td>\n      <td>0.913793</td>\n      <td>0.980000</td>\n    </tr>\n    <tr>\n      <th>Random Forest</th>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.962963</td>\n      <td>0.982759</td>\n      <td>0.993333</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Therefore, Random Forest Classifier gives the highest accuracy with a tie of 1.00 with random sampling\n# and stratified sampling.","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}